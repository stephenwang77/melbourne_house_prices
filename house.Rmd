---
title: "Personal Income"
author: "Stephen Wang"
date: "25/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) #data manipulation
library(tidyr) #tidying data 
library(readr) #reading files 
library(ggplot2) #data visualisation
library(purrr) #toolkit for functions and vectors
library(stringr) #string manipulation
library(reshape2) #reshaping data
library(ggcorrplot) #correlation matrixes
```

There have been recent speculation of a house bubble about to burst in Oceania (Australia and NZ). With low interest rates and a declining house economy, this could be a once in a lifetime opportunity. For this project, I wanted to investigate the factors that affect house prices. 

Hypothesis: the variables that affects the price the most is location (distance from CBD), number of rooms, and landsize. 

The data was downlodaded from: https://www.kaggle.com/anthonypino/melbourne-housing-market#Melbourne_housing_FULL.csv


```{r}
filename = "melbourne_house_prices.csv"
house_price <- read_csv(filename)
dim(house_price)
head(house_price, 10)
tail(house_price, 10)
summary(house_price)
```

Dimension: 34,857 rows x 21 columns 

Since there are 21 columns, I will just be picking the variables relevant to our study. 

Rooms: total rooms in house 
Price: price sold 
Distance: distance from cbd
Bedroom2: total number of bedrooms 
Bathroom: total number of bathrooms
Car: number of cars that can be parked
Landsize: landsize in m2
BuildingArea: building area in m2
YearBuilt: year constructed
Postcode/CouncilArea: these two are related; each postcode is associated with a council area 
Method: s =, sa =, sp = vb = 
Type: t = townhouse, u = unit 

Observations:
- Minimum price is $85,000 so that's good - there's no 0 value houses. 
- Maximum price is a whopping $11,200,000. 
- If we take a look at the histogram of distance, majority of houses are between 6.4km and 14km from CBD. Straight off the bat, this means that houses in rural areas are not on the market or not being sold or a biased data collection. 
- Rooms are defined to be used for habitation while bedrooms are intended for sleeping. Rooms include bedrooms, kitchens, living rooms, etc. The distinction between rooms and bedrooms is defined here: http://archive.stats.govt.nz/methods/classifications-and-standards/classification-related-stats-standards/number-of-rooms-bedrooms/definition.aspx


```{r}
# Cleaning up some data  
house_price$Postcode <- as.factor(house_price$Postcode)
```


```{r}
# Checking for NA values
round(colSums(is.na(house_price)) / nrow(house_price) * 100, digits = 2)
```

This shows the proportion of missing data (NA values) in our dataset. 

Before we continue, let's understand the nature of missing data values. 
(a) Why does it occur?
(b) How can this affect the study?
(c) What can we do?

Missing values can be a result of multiple factors. The data source may not actually have some of the data, people being surveyed don't want to disclose some personal information, some variables don't satisfy the data e.g. apartment don't have landsize. 

There are three types of missing data:
MCAR - missing completely at random
MAR - missing at random
MNAR - missing not at random

Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions. 

What solutions do we have?
(1) Deletion: Remove rows with null variables
(2) Multiple Imputation: Replacing null values with plausible values based on the correlations for the missing data and then averages the simulated datasets by incorporating random errors in your predictions

From our null value proportions, we can noticeably see that some of the variables have too much missing data values. Building area has 60.58% missing data, YearBuilt has 55.39% missing data, etc, and these variables are argubly important for regression. 

Reference: 
- https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/
- https://measuringu.com/handle-missing-data/#targetText=Use%20caution%20unless%20you%20have,value%20from%20the%20other%20values
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/#targetText=Missing%20data%20can%20reduce%20the,estimates%2C%20leading%20to%20invalid%20conclusions.&targetText=The%20problem%20of%20missing%20data,from%20the%20data%20%5B1%5D.



```{r}
null_rows <- house_price %>% filter(is.na(house_price$Price))
removed_null_price <- house_price %>% select(-YearBuilt) %>% filter(!is.na(house_price$Price), !is.na(house_price$Landsize), !is.na(house_price$BuildingArea))  
round(colSums(is.na(removed_null_price)) / nrow(removed_null_price) * 100, digits = 2)
```

21.83% of data has missing values for price which is beyond the 5% threshold to imputate values. So we're going to remove rows with NA values. (We will do a time-series analysis later so we are going to remove the YearBuilt column.)

After deletion of data, our dimension is 9382 rows x 20 columns. 

```{r}
ggplot(data=removed_null_price,
       aes(Price))+
  geom_histogram()

# removed_null_price <- removed_null_price %>% mutate(Price=log10(Price))

ggplot(data=removed_null_price,
       aes(Price))+
  geom_histogram()
   
```

Our variable of interest (or dependent variable) is Price so I wanted to check out the price distribution of this dataset. The data is right skewed so we need to log transform price to fix the skewness. This is also better for our study because we are interested in percentage change. 

```{r}
numeric_data <- removed_null_price %>% select_if(is.numeric)
corr <- numeric_data %>% cor(use="pairwise.complete.obs")
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   outline.col = "black",
   colors = c("#6D9EC1", "white", "#E46726"),
   insig = "blank")

#ggplot(data=removed_null_price,
       #aes(x=as.factor(Rooms), y=Price))+
  #geom_boxplot()
```

There are two personal conundrums for this investigation: 
(1) I need to understand the fundamentals of multiple linear regression
(2) And I need to figure out the difference between a generalised linear model (glm) and linear model (lm) for R programming. 

Multiple linear regression involves fitting two or more explanatory variables to explain the response variable, in this case, Melbourne house prices. We need to first check the scatter plot of our variables to see if there is a discernible linear relationship. 

The lm (linear model) fits the model in the form of y = xb + e where e~iid(0, s^2)
The glm fits the model in the form of g(y) = xb + e where the function g and e needs to be specified. This is known as the link function which transforms a non-linear regression to linear. 

References:
https://garrettgman.github.io/model-fitting/
http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm
https://www.reddit.com/r/rstats/comments/2izyw1/difference_between_glm_and_lm_lmyxz_and_glmyxz/


```{r}
selected <- numeric_data[,c(1:3, 5:7)]
selected$Price <- log10(selected$Price)
plot(selected)
```

```{r}
linear_model <- lm(formula=Price~Rooms+Bathroom+Distance+Bedroom2+Car, data=selected)
summary(linear_model)
confint(linear_model)
```

In this scenario, I logged price seperately and fitted a linear regression model using a temporary dataframe. 
The selected variables and fitted model are all statistically significant but our R-squared value suggests that our model can only explain 46.85% of variance for house prices. 

```{r}
# Attempt number two: putting all the variables 
linear_model_2 <- lm(formula=log(Price)~Rooms+Bathroom+Distance+Bedroom2+Car+CouncilArea+Type+Method, data=removed_null_price)
summary(linear_model_2)
confint(linear_model_2)
```

After we included some more variables to the linear model equation, the R-squared value increased from 0.4682 to 0.762. However, there were three council areas (Hobson Bay, Moorabool Shire, and Millumbik Shire) and two methods (SA and VB) with no statistical significance. I wanted to take a closer look at these two variables. 

```{r}
flagged_council <- removed_null_price %>% mutate(flag=
                                                    ifelse(CouncilArea=="Hobsons Bay City Council" | CouncilArea=="Moorabool Shire Council" | CouncilArea=="Nillumbik Shire Council", "TRUE", "FALSE"))

ggplot(data=flagged_council,
       aes(CouncilArea,
           fill=flag))+
   geom_bar(stat="count")+
   theme(axis.text.x=element_text(angle=45,hjust=1))
   scale_fill_manual(c("TRUE"="red", "FALSE"="black"),guide=FALSE)
```

Small sample sizes make it harder to find statistical significance which is present in Moorabool Shire and Nillumbik Shire. Hobson Bay has an approximate 300 sample size so we can confirm that this location doesn't have statistical signifiance - the price when taking Hobson Bay into account is attributed to chance. 

The article suggests that "when we do find statistical significance with small sample sizes, the differences are large and more likely to drive action". Sometimes statistical significance does not translate to practical significance i.e. a miniscule effect on the dependent variable. 

Reference:
https://measuringu.com/statistically-significant/#targetText=Not%20Due%20to%20Chance,result%20that%20large%20or%20larger
https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/



Summary: 
It's hard to imagine how there is a low correlation between Landsize/BuildingArea and price. 



Note to self:
select() -> columns
filter() -> rows 
